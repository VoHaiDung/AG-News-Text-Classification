{
"nbformat": 4,
"nbformat_minor": 5,
"metadata": {
"kernelspec": {
"display_name": "Python 3",
"language": "python",
"name": "python3"
},
"language_info": {
"name": "python",
"version": "3.10"
}
},
"cells": [
{
"cell_type": "markdown",
"metadata": {},
"source": [
"# Error Analysis & Explainability\n",
"In this notebook, we load misclassified examples, visualize attention heatmaps, and generate SHAP explanations to understand model behavior."]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"source": [
"import pandas as pd\n",
"# Load misclassified examples\n",
"errors = pd.read_csv('outputs/metrics/error_cases.csv')\n",
"# Display first few error cases\n",
"errors.head()"
],
"outputs": []
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"source": [
"from src.explainability import plot_attention\n",
"import matplotlib.pyplot as plt\n",
"# Visualize attention heatmap for the first error sample\n",
"text = errors.iloc[0]['text']\n",
"print(f'Input text: {text}')\n",
"heatmap = plot_attention(text)\n",
"plt.imshow(heatmap, aspect='auto')\n",
"plt.title('Attention Heatmap')\n",
"plt.xlabel('Token Position')\n",
"plt.ylabel('Token Position')\n",
"plt.colorbar()\n",
"plt.show()"
],
"outputs": []
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"source": [
"from src.explainability import compute_shap_values\n",
"import shap\n",
"# Compute SHAP values for the first error example\n",
"model, tokenizer = compute_shap_values()\n",
"inputs = tokenizer(text, return_tensors='pt')\n",
"shap_values = model.explain(inputs)  # assumes explain returns SHAP values\n",
"# Plot SHAP summary for the example\n",
"shap.force_plot(shap_values.base_values, shap_values.values, inputs['input_ids'], matplotlib=True)"
],
"outputs": []
}
]
}
