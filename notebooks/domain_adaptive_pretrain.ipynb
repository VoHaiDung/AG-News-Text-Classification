{
"nbformat": 4,
"nbformat_minor": 5,
"metadata": {
"kernelspec": {
"display_name": "Python 3",
"name": "python3",
"language": "python"
},
"language_info": {
"name": "python",
"version": "3.10"
}
},
"cells": [
{
"cell_type": "markdown",
"metadata": {},
"source": [
"# Domain‑Adaptive Pretraining (DAPT)\n",
"This notebook demonstrates how to continue Masked Language Model pretraining for DeBERTa‑v3‑large on an unlabeled news corpus.\n",
"\n",
"> Reference: Gururangan et al. (2020), "Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks.""
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"source": [
"# Install dependencies (first run only)\n",
"!pip install -q transformers datasets torch accelerate peft evaluate pandas numpy matplotlib"
],
"outputs": []
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"source": [
"# Optional: mount Google Drive to save/load checkpoints\n",
"# from google.colab import drive\n",
"# drive.mount('/content/drive')"
],
"outputs": []
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"source": [
"import os\n",
"import torch\n",
"from src.pretrain_lm import run_dapt\n",
"from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
"from datasets import load_dataset\n",
"import math\n",
"\n",
"# Configuration parameters\n",
"MODEL_NAME     = 'microsoft/deberta-v3-large'  # Base MLM model\n",
"DATA_FILE      = 'data/external/unlabeled.txt' # Unlabeled news corpus\n",
"OUTPUT_DIR     = 'outputs/dapt_checkpoints/'   # Where to save adapted model\n",
"NUM_EPOCHS     = 5                             # Number of DAPT epochs\n",
"BATCH_SIZE     = 8                             # Batch size for DAPT\n",
"LEARNING_RATE  = 5e-5                          # Learning rate for DAPT\n",
"BLOCK_SIZE     = 512                           # Sequence length for grouping\n",
"MLM_PROB       = 0.15                          # Masking probability\n",
"DEVICE         = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
"\n",
"# Ensure output directory exists\n",
"os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
"\n",
"# 1. Load tokenizer and model\n",
"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
"model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
"\n",
"# 2. Prepare unlabeled dataset\n",
"dataset = load_dataset('text', data_files={'train': DATA_FILE})['train']\n",
"\n",
"# 3. Run DAPT using run_dapt helper\n",
"run_dapt(\n",
"    model_name=MODEL_NAME,\n",
"    data_file=DATA_FILE,\n",
"    output_dir=OUTPUT_DIR,\n",
"    num_train_epochs=NUM_EPOCHS,\n",
"    batch_size=BATCH_SIZE,\n",
"    learning_rate=LEARNING_RATE,\n",
")\n"
],
"outputs": []
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"source": [
"# 4. Quick perplexity evaluation on DAPT dataset\n",
"from torch.utils.data import DataLoader\n",
"from transformers import DataCollatorForLanguageModeling\n",
"\n",
"# Reload model and tokenizer from checkpoint\n",
"tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR, use_fast=True)\n",
"model = AutoModelForMaskedLM.from_pretrained(OUTPUT_DIR).to(DEVICE)\n",
"\n",
"# Tokenize and group texts into blocks\n",
"def tokenize_and_group(examples):\n",
"    tokenized = tokenizer(\n",
"        examples['text'],\n",
"        return_special_tokens_mask=True,\n",
"        truncation=True,\n",
"        max_length=BLOCK_SIZE\n",
"    )\n",
"    concatenated = {k: sum(tokenized[k], []) for k in tokenized.keys()}\n",
"    total_len = (len(concatenated['input_ids']) // BLOCK_SIZE) * BLOCK_SIZE\n",
"    result = {\n",
"        k: [t[i:i+BLOCK_SIZE] for i in range(0, total_len, BLOCK_SIZE)]\n",
"        for k, t in concatenated.items()\n",
"    }\n",
"    return result\n",
"\n",
"blocks = dataset.map(tokenize_and_group, batched=True, remove_columns=['text'])\n",
"collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=MLM_PROB)\n",
"loader = DataLoader(blocks, batch_size=BATCH_SIZE, collate_fn=collator)\n",
"\n",
"# Compute perplexity\n",
"model.eval()\n",
"total_loss = 0.0\n",
"for batch in loader:\n",
"    batch = {k: v.to(DEVICE) for k,v in batch.items()}\n",
"    with torch.no_grad():\n",
"        outputs = model(**batch)\n",
"    total_loss += outputs.loss.item()\n",
"avg_loss = total_loss / len(loader)\n",
"perplexity = math.exp(avg_loss)\n",
"print(f'Average loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}')"
],
"outputs": []
}
]
}
