{
"nbformat": 5,
"nbformat\_minor": 10,
"metadata": {
"kernelspec": {
"display\_name": "Python 3",
"language": "python",
"name": "python3"
},
"language\_info": {
"name": "python",
"version": "3.10"
}
},
"cells": \[
{
"cell\_type": "markdown",
"metadata": {},
"source": \[
"# Domain‑Adaptive Pretraining (DAPT)\n",
"This notebook demonstrates how to continue masked‑language model pretraining for `DeBERTa‑v3‑large` on an unlabeled news corpus.\n",
"> Reference: Gururangan et al. (2020), “Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks.”"
]
},
{
"cell\_type": "code",
"execution\_count": null,
"metadata": {},
"source": \[
"# Install dependencies (first run only)\n",
"!pip install -q transformers datasets torch accelerate peft"
],
"outputs": \[]
},
{
"cell\_type": "code",
"execution\_count": null,
"metadata": {},
"source": \[
"# (Optional) Mount Google Drive to save checkpoints\n",
"# from google.colab import drive\n",
"# drive.mount('/content/drive')"
],
"outputs": \[]
},
{
"cell\_type": "code",
"execution\_count": null,
"metadata": {},
"source": \[
"import os\n",
"from src.pretrain\_lm import run\_dapt\n",
"\n",
"# Configuration parameters\n",
"MODEL\_NAME     = 'microsoft/deberta-v3-large'\n",
"DATA\_FILE      = 'data/external/unlabeled.txt'\n",
"OUTPUT\_DIR     = 'outputs/dapt\_checkpoints/'\n",
"NUM\_EPOCHS     = 3        # Increase to 5–10 for production\n",
"BATCH\_SIZE     = 4        # Smaller batch on Colab GPU\n",
"LEARNING\_RATE  = 5e-5\n",
"\n",
"# Ensure output directory exists\n",
"os.makedirs(OUTPUT\_DIR, exist\_ok=True)\n",
"\n",
"# Run domain‑adaptive pretraining\n",
"run\_dapt(\n",
"    model\_name=MODEL\_NAME,\n",
"    data\_file=DATA\_FILE,\n",
"    output\_dir=OUTPUT\_DIR,\n",
"    num\_train\_epochs=NUM\_EPOCHS,\n",
"    batch\_size=BATCH\_SIZE,\n",
"    learning\_rate=LEARNING\_RATE,\n",
")"
],
"outputs": \[]
},
{
"cell\_type": "markdown",
"metadata": {},
"source": \[
"After training completes, the adapted checkpoint is saved to **`outputs/dapt_checkpoints/`**.\n",
"You can now fine‑tune this checkpoint with LoRA by pointing your training script at that directory (e.g. `--model_name_or_path outputs/dapt_checkpoints/`)."
]
}
]
}
