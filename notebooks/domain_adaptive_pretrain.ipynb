{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Domain‑Adaptive Pretraining (DAPT)\n",
        "\n",
        "**This notebook demonstrates how to continue masked‑language model pretraining for** `DeBERTa‑v3‑large` **on an unlabeled news corpus.**\n",
        "\n",
        "> Reference: Gururangan et al. (2020), “Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks.”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies (only needed first time in Colab)\n",
        "!pip install -q transformers datasets torch accelerate peft"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional: mount Google Drive to save or load checkpoints\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "from src.pretrain_lm import run_dapt\n",
        "\n",
        "# Configuration parameters\n",
        "MODEL_NAME     = 'microsoft/deberta-v3-large'\n",
        "DATA_FILE      = 'data/external/unlabeled.txt'\n",
        "OUTPUT_DIR     = 'outputs/dapt_checkpoints/'\n",
        "NUM_EPOCHS     = 3        # Increase to 5–10 for production\n",
        "BATCH_SIZE     = 4        # Smaller batch on Colab GPU\n",
        "LEARNING_RATE  = 5e-5\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Run domain‑adaptive pretraining\n",
        "run_dapt(\n",
        "    model_name=MODEL_NAME,\n",
        "    data_file=DATA_FILE,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After training completes, the adapted checkpoint is saved to **`outputs/dapt_checkpoints/`**.  \n",
        "You can now fine‑tune this checkpoint with LoRA by pointing your training script at that directory (e.g. `--model_name_or_path outputs/dapt_checkpoints/`)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
