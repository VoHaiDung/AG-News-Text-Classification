{
"nbformat": 4,
"nbformat_minor": 5,
"metadata": {
"kernelspec": {
"display_name": "Python 3",
"language": "python",
"name": "python3"
},
"language_info": {
"name": "python",
"version": "3.10"
}
},
"cells": [
{
"cell_type": "markdown",
"metadata": {},
"source": [
"# Domain‑Adaptive Pretraining (DAPT)\n",
"This notebook demonstrates how to continue Masked Language Model pretraining for DeBERTa‑v3‑large on an unlabeled news corpus.\n",
"\n",
"> Reference: Gururangan et al. (2020), 'Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks.'"
]
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"source": [
"# Install dependencies (first run only)\n",
"!pip install -q transformers datasets torch accelerate peft evaluate pandas numpy matplotlib"
],
"outputs": []
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"source": [
"# (Optional) Mount Google Drive to save/load checkpoints\n",
"# from google.colab import drive\n",
"# drive.mount('/content/drive')"
],
"outputs": []
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"source": [
"import os\n",
"import math\n",
"import torch\n",
"from src.pretrain_lm import run_dapt\n",
"from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
"from datasets import load_dataset\n",
"\n",
"# Configuration parameters\n",
"MODEL_NAME    = 'microsoft/deberta-v3-large'   # Base MLM model\n",
"DATA_FILE     = 'data/external/unlabeled.txt'  # Unlabeled news corpus\n",
"OUTPUT_DIR    = 'outputs/dapt_checkpoints/'    # Save adapted model here\n",
"NUM_EPOCHS    = 5                              # Number of DAPT epochs\n",
"BATCH_SIZE    = 8                              # Batch size for DAPT\n",
"LEARNING_RATE = 5e-5                           # Learning rate for DAPT\n",
"BLOCK_SIZE    = 512                            # Sequence length for grouping\n",
"MLM_PROB      = 0.15                           # Masking probability\n",
"DEVICE        = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
"\n",
"# Confirm data file exists\n",
"assert os.path.exists(DATA_FILE), f\"Data file not found: {DATA_FILE}\"",
"os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
"\n",
"# 1. Run Domain‑Adaptive Pretraining\n",
"run_dapt(\n",
"    model_name=MODEL_NAME,\n",
"    data_file=DATA_FILE,\n",
"    output_dir=OUTPUT_DIR,\n",
"    num_train_epochs=NUM_EPOCHS,\n",
"    batch_size=BATCH_SIZE,\n",
"    learning_rate=LEARNING_RATE,\n",
")"
],
"outputs": []
},
{
"cell_type": "code",
"execution_count": null,
"metadata": {},
"source": [
"# 2. Evaluate Perplexity of Adapted Model\n",
"import pandas as pd\n",
"from torch.utils.data import DataLoader\n",
"from transformers import DataCollatorForLanguageModeling\n",
"# Reload adapted model/tokenizer\n",
"tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR, use_fast=True)\n",
"model = AutoModelForMaskedLM.from_pretrained(OUTPUT_DIR).to(DEVICE)\n",
"# Load raw dataset again to avoid mutation\n",
"raw_ds = load_dataset('text', data_files={'train': DATA_FILE})['train']\n",
"# Tokenization & grouping function\n",
"def tokenize_and_group(examples):\n",
"    tok = tokenizer(\n",
"        examples['text'],\n",
"        return_special_tokens_mask=True,\n",
"        truncation=True,\n",
"        padding='max_length',\n",
"        max_length=BLOCK_SIZE\n",
"    )\n",
"    all_ids = tok['input_ids']\n",
"    # Group into blocks of size BLOCK_SIZE\n",
"    return {'input_ids': [all_ids[i:i+BLOCK_SIZE] for i in range(0, len(all_ids), BLOCK_SIZE)]}\n",
"blocks = raw_ds.map(tokenize_and_group, batched=True, remove_columns=['text'])\n",
"collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=MLM_PROB)\n",
"loader = DataLoader(blocks, batch_size=BATCH_SIZE, collate_fn=collator)\n",
"# Compute perplexity\n",
"model.eval()\n",
"total_loss = 0.0\n",
"for batch in loader:\n",
"    batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
"    with torch.no_grad():\n",
"        outputs = model(**batch)\n",
"    total_loss += outputs.loss.item()\n",
"avg_loss = total_loss / len(loader)\n",
"print(f'Average Loss: {avg_loss:.4f}\nPerplexity: {math.exp(avg_loss):.2f}')"
],
"outputs": []
}
]
}
