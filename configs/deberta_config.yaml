# Fine‑tune DeBERTa‑v3‑large with LoRA adapters
model:
  base_model: microsoft/deberta-v3-large
  num_labels: 4

tokenization:
  max_length: 512
  stride: 256

training:
  epochs: 5
  batch_size: 16
  learning_rate: 2e-5
  weight_decay: 0.01
  gradient_accumulation_steps: 2
  warmup_ratio: 0.1
  fp16: true

seed: 42

output:
  model_dir: results/deberta_lora
  logging_dir: results/logs/deberta
